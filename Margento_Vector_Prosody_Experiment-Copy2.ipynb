{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christanasescu/fastText_multilingual-master\n"
     ]
    }
   ],
   "source": [
    "cd fastText_multilingual-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import FastVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastVector:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "            (self.n_words, self.n_dim) = \\\n",
    "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "\n",
    "    def apply_transform(self, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)\n",
    "\n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "\n",
    "    def translate_nearest_neighbour(self, source_vector):\n",
    "        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n",
    "        similarity_vector = np.matmul(FastVector.normalised(self.embed), source_vector)\n",
    "        target_id = np.argmax(similarity_vector)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def translate_inverted_softmax(self, source_vector, source_space, nsamples,\n",
    "                                   beta=10., batch_size=100, recalculate=True):\n",
    "        \"\"\"\n",
    "        Obtain translation of source_vector using sampled inverted softmax retrieval\n",
    "        with inverse temperature beta.\n",
    "        nsamples vectors are drawn from source_space in batches of batch_size\n",
    "        to calculate the inverted softmax denominators.\n",
    "        Denominators from previous call are reused if recalculate=False. This saves\n",
    "        time if multiple words are translated from the same source language.\n",
    "        \"\"\"\n",
    "        embed_normalised = FastVector.normalised(self.embed)\n",
    "        # calculate contributions to softmax denominators in batches\n",
    "        # to save memory\n",
    "        if self.softmax_denominators is None or recalculate is True:\n",
    "            self.softmax_denominators = np.zeros(self.embed.shape[0])\n",
    "            while nsamples > 0:\n",
    "                # get batch of randomly sampled vectors from source space\n",
    "                sample_vectors = source_space.get_samples(min(nsamples, batch_size))\n",
    "                # calculate cosine similarities between sampled vectors and\n",
    "                # all vectors in the target space\n",
    "                sample_similarities = \\\n",
    "                    np.matmul(embed_normalised,\n",
    "                              FastVector.normalised(sample_vectors).transpose())\n",
    "                # accumulate contribution to denominators\n",
    "                self.softmax_denominators \\\n",
    "                    += np.sum(np.exp(beta * sample_similarities), axis=1)\n",
    "                nsamples -= batch_size\n",
    "        # cosine similarities between source_vector and all target vectors\n",
    "        similarity_vector = np.matmul(embed_normalised,\n",
    "                                      source_vector/np.linalg.norm(source_vector))\n",
    "        # exponentiate and normalise with denominators to obtain inverted softmax\n",
    "        softmax_scores = np.exp(beta * similarity_vector) / \\\n",
    "                         self.softmax_denominators\n",
    "        # pick highest score as translation\n",
    "        target_id = np.argmax(softmax_scores)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def get_samples(self, nsamples):\n",
    "        \"\"\"Return a matrix of nsamples randomly sampled vectors from embed\"\"\"\n",
    "        sample_ids = np.random.choice(self.embed.shape[0], nsamples, replace=False)\n",
    "        return self.embed[sample_ids]\n",
    "\n",
    "    @classmethod\n",
    "    def normalised(cls, mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.en.vec\n"
     ]
    }
   ],
   "source": [
    "# fr_dictionary = FastVector(vector_file='wiki.fr.vec')\n",
    "en_dictionary = FastVector(vector_file='wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr_words = set(fr_dictionary.word2id.keys())\n",
    "en_words = set(en_dictionary.word2id.keys())\n",
    "# overlap = list(en_words & fr_words)\n",
    "# bilingual_dictionary = [(entry, entry) for entry in overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = list(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the training matrices\n",
    "# source_matrix, target_matrix = make_training_matrices(\n",
    "    # fr_dictionary, en_dictionary, bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn and apply the transformation\n",
    "# transform = learn_transformation(source_matrix, target_matrix)\n",
    "# fr_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see above 'chien' and 'canis' are now much more similar than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "#The OS module in Python provides a way of using operating system dependent functionality. \n",
    "#The functions that the OS module provides allows you to interface with the underlying operating system \n",
    "#that Python is running on – be that Windows, Mac or Linux.\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim import models, corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christanasescu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk as nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = nltk.corpus.stopwords.words(\"stopwords_Latin.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = _pre_clean(tokens)\n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_clean(list_of_text):\n",
    "        '''\n",
    "        preliminary cleaning of the text\n",
    "        - remove new line character i.e. \\n or \\r\n",
    "        - remove tabs i.e. \\t\n",
    "        - remove extra spaces\n",
    "        '''\n",
    "        cleaned_list = []\n",
    "        for text in list_of_text:\n",
    "            # print(\"original:\", text)\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            text = text.replace('\\\\r', ' ')\n",
    "            text = text.replace('\\\\t', ' ')\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "            text = text.strip()\n",
    "            text = text.lower()\n",
    "            # check for empty strings\n",
    "            if text != '' and text is not None:\n",
    "                cleaned_list.append(text)\n",
    "\n",
    "        return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filelabels = {}\n",
    "\n",
    "#import re as re \n",
    "\n",
    "#def get_documents(path):\n",
    "    #os.chdir(path)\n",
    "    #files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    #texts = []\n",
    "    #count = -1\n",
    "    #for f in files:\n",
    "        #with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "            #count = count + 1\n",
    "            #filelabels[count] = os.path.basename(openf.name)\n",
    "            #splitted_lines = openf.read().splitlines()\n",
    "            #splitted_lines = _pre_clean(splitted_lines)\n",
    "            #texts.append(splitted_lines)\n",
    "    #print(filelabels)\n",
    "    #return texts\n",
    "\n",
    "#TEXTS_DIR = HOME + \"/UK_EN/\"\n",
    "\n",
    "#documents = get_documents(TEXTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = nltk.corpus.stopwords.words('stop_words_poetry.txt')\n",
    "\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"'d\")\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"&\")\n",
    "stopwords.append(\"upon\")\n",
    "stopwords.append(\"also\")\n",
    "stopwords.append(\"hath\")\n",
    "stopwords.append(\"must\")\n",
    "stopwords.append(\"therefore\")\n",
    "stopwords.append(\"doth\")\n",
    "stopwords.append(\"could\")\n",
    "stopwords.append(\"would\")\n",
    "#stopwords.append(\"another\")\n",
    "stopwords.append(\"much\")\n",
    "#stopwords.append(\"give\")\n",
    "stopwords.append(\"like\")\n",
    "stopwords.append(\"since\")\n",
    "#stopwords.append(\"many\")\n",
    "#stopwords.append(\"without\")\n",
    "#stopwords.append(\"first\")\n",
    "stopwords.append(\"though\")\n",
    "#stopwords.append(\"well\")\n",
    "#stopwords.append(\"often\")\n",
    "#stopwords.append(\"great\")\n",
    "stopwords.append(\"either\")\n",
    "#stopwords.append(\"even\")\n",
    "stopwords.append(\"shall\")\n",
    "#stopwords.append(\"they\")\n",
    "stopwords.append(\"what\")\n",
    "stopwords.append(\"their\")\n",
    "#stopwords.append(\"more\")\n",
    "#stopwords.append(\"there\")\n",
    "#stopwords.append(\"your\")\n",
    "#stopwords.append(\"them\")\n",
    "stopwords.append(\"’\")\n",
    "stopwords.append(\"“\")\n",
    "stopwords.append(\"2\")\n",
    "stopwords.append(\"3\")\n",
    "stopwords.append(\"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['a', 'like', 'you', 'they', 'he', 'be', 'it', 'your', 'her', 'of', 'more', 'there', 'no', 'not', '’', 'what', 'my', 'his', 'she', 'to', 'our', 'me', 'we', 'in', 'can', 'us', 'an', 'if', 'do', 'this', '”', 'because', 'who', 'hand', 'but', 'him'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'charles_wright_apologia_pro_vita_sua.txt', 1: 'elke_de_rijcke_deffectiveness.txt', 2: 'martinus_nijhoff_trans_james_s_holmes_awater_excerpt.txt', 3: 'place_andreea_scridon_bucharest_cartography_romania.txt', 4: 'place_babylonians-trans-jared-pearce_high-priest-prayer.txt', 5: 'place_david-baker_weed_granville-ohio-&-ontario-&-illinois-&-eerie-&-carolina-&-asia.txt', 6: 'place_liliane_wouters_trans_margento_a_pascal_bill_brabant_flanders_belgium.txt', 7: 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 8: 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 9: 'sharon_olds_the_knowing.txt', 10: 'ted_hughes_harvest_moon.txt', 11: 'verheggen_portrait_trans_taylor.txt', 12: 'veronique_bergen_trans_margento_wolves.txt', 13: 'w_h_auden_brussels_in_winter.txt'}\n"
     ]
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "\n",
    "TEXTS_DIR = HOME + \"/vector_prosody_experiment/\"\n",
    "\n",
    "#TEXTS_DIR = HOME\n",
    "\n",
    "filelabels_en = {}\n",
    "\n",
    "texts_data = []\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "tokens_total = []\n",
    "\n",
    "count = -1\n",
    " \n",
    "os.chdir(TEXTS_DIR)\n",
    "    \n",
    "for f in files:\n",
    "    #os.chdir(TEXTS_DIR)\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        tokens = []\n",
    "        count = count + 1\n",
    "        filelabels_en[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "            sent_text = nltk.sent_tokenize(line)\n",
    "            for sentence in sent_text:\n",
    "                tokens1 = tokenize(sentence)\n",
    "                tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "                #filter_object = filter(lambda x: x != \"\", tokens1)\n",
    "                tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "                for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "                    tokens_total.append(token)\n",
    "                #if random.random() > .99:\n",
    "                #print(tokens)\n",
    "    #print(tokens_total)\n",
    "    texts_data.append(tokens)\n",
    "\n",
    "print(filelabels_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelabels1 = list(filelabels_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'charles_wright_apologia_pro_vita_sua.txt', 1: 'elke_de_rijcke_deffectiveness.txt', 2: 'place_babylonians-trans-jared-pearce_high-priest-prayer.txt', 3: 'place_david-baker_weed_granville-ohio-&-ontario-&-illinois-&-eerie-&-carolina-&-asia.txt', 4: 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 5: 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 6: 'sharon_olds_the_knowing.txt', 7: 'ted_hughes_harvest_moon.txt', 8: 'veronique_bergen_trans_margento_wolves.txt', 9: 'w_h_auden_brussels_in_winter.txt'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(filelabels_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "playlist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charles wright, \"apologia pro vita sua\" (vect_en[0])\n",
    "rachel blau duplessis, \"draft 56: that\" ([4]) \n",
    "\"old babylonian contracts. loan of barley, from a temple,\" trans. jared pierce ([2])\n",
    "w.h. auden, \"brussels in winter\" ([9]) \n",
    "david baker, \"weed\" ([3])\n",
    "veronique bergen, \"wolfpack\" ([8])\n",
    "elke de rijcke, trans. margento, \"my defectiveness facilitated your ground. findings of an emancipated woman. thank you (in the kitchen)\" ([1])\n",
    "sappho (feat. walt whitman), multiple trans. ([5])\n",
    "sharon olds, \"the knowing\" ([6])\n",
    "ted hughes, \"harvest moon\" ([7])\n",
    "jean-pierre verheggen, trans. john taylor, \"self-portrait\"\n",
    "liliane wouters, trans. margento, \"the pascal bill\"\n",
    "martinus nijhoff, trans. james s. holmes, \"awater\" (excerpt)\n",
    "andreea iulia scridon, \"bucharest cartography (sketch)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filelabels1)):\n",
    "    texts_data[i] = [x for x in texts_data[i] if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(x):\n",
    "   return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def div_norm(x):\n",
    "   norm_value = l2_norm(x)\n",
    "   if norm_value > 0:\n",
    "       return x * ( 1.0 / norm_value)\n",
    "   else:\n",
    "       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 partcharred\n",
      "0 worldweight\n",
      "0 sapcrippled\n",
      "0 winterweathered\n",
      "0 jackwedge\n",
      "0 boutonnieres\n",
      "0 disfecemi\n",
      "0 blossomstarred\n",
      "0 starcrystals\n",
      "0 perchio\n",
      "0 ballatetta\n",
      "0 thingless\n",
      "0 thingless\n",
      "0 engenderer\n",
      "0 onlybegetter\n",
      "0 halfclerk\n",
      "0 greenleafed\n",
      "1 intumescences\n",
      "1 ​​progression\n",
      "2 fanblade\n",
      "2 soldieress\n",
      "2 loosetied\n",
      "2 enswathed\n",
      "3 autumned\n",
      "3 greataunttanti\n",
      "3 30s\n",
      "3 valentinecard\n",
      "3 doublestrapped\n",
      "3 slickedback\n",
      "3 wroughtiron\n",
      "3 70s\n",
      "3 driedup\n",
      "3 paintedblack\n",
      "3 altamahaha\n",
      "3 1988\n",
      "3 onionbulbed\n",
      "3 hexagonal…\n",
      "3 14sectored\n",
      "3 dirtgrey\n",
      "3 dirtblue\n",
      "3 eighthour\n",
      "3 sixteen…leave\n",
      "4 damuribam\n",
      "4 urdatum\n",
      "5 ashgreen\n",
      "5 graygreen\n",
      "5 threebranching\n",
      "5 footlongat\n",
      "5 2002\n",
      "5 75\n",
      "5 4\n",
      "6 allsnowy\n",
      "7 65\n",
      "7 no—possessive\n",
      "7 halfturned\n",
      "7 newlydead\n",
      "7 alterest\n",
      "7 sdstated\n",
      "8 sallower\n",
      "8 wellpossess\n",
      "8 undissuadable\n",
      "8 pentup\n",
      "8 bestbeloved\n",
      "8 lovespendings\n",
      "9 comaed\n",
      "9 bluegreygreen\n",
      "9 selfregard\n",
      "9 nonnomadic\n",
      "10 flamered\n",
      "11 faultedhouse\n",
      "11 hercullion\n",
      "11 renaud—in\n",
      "11 tittylon\n",
      "11 1st\n",
      "11 1942\n",
      "11 20th\n",
      "12 hyperconsciousness\n",
      "12 ethylism\n",
      "12 transgrammatical\n",
      "12 necromants\n"
     ]
    }
   ],
   "source": [
    "vect_en = []\n",
    "\n",
    "for i in range(len(filelabels1)):\n",
    "        vect1 = []\n",
    "        for j in range(len(texts_data[i])):\n",
    "            if texts_data[i][j] in en_words:\n",
    "                vect1.append(div_norm(en_dictionary[texts_data[i][j]]))\n",
    "            else:\n",
    "                print(i, texts_data[i][j])\n",
    "                continue\n",
    "        vect0 = sum(vect1) / len(texts_data[i])\n",
    "        vect_en.append(vect0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(vect_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(x):\n",
    "    vect1 = []\n",
    "    for j in range(len(x)):\n",
    "            if x[j] in stopwords:\n",
    "                continue\n",
    "            else:\n",
    "                if x[j] in en_words:\n",
    "                    vect1.append(div_norm(en_dictionary[x[j]]))\n",
    "                else:\n",
    "                    print(x[j])\n",
    "                    continue\n",
    "    vect0 = sum(vect1) / len(x)\n",
    "    return vect0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(x,y):\n",
    "    return dot(x, y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BOW (bag of words) poetry; not only reading and writing as [not like] an algorithm (Drucker 2021), \n",
    "# but also writing for an algorithm reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# line0 = ['road', 'end', 'cobble', 'drunkenness', 'nowhere', 'go', 'keep', 'on', 'eudore', 'pirmez', 'like', 'past', 'clouds', 'tight', 'passage', 'above', 'street', 'stones', 'seen', 'world', 'starting', 'tilt', 'nowhere', 'where', 'were', 'going', 'visible', 'thing', 'thingless', 'we', 'came', 'world', 'thingless', 'we', 'leave', 'still', 'waves', 'bankrupt', 'pubs', 'suicides', 'eye','attracted', 'downward', 'down', 'avenue', 'malou', 'soon', 'killed', 'assimilated', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line0 = ['road', 'end', 'cobble', 'drunkenness', 'nowhere', 'go', 'keep', 'on', 'eudore', 'pirmez', 'like', 'past', 'clouds', 'tight', 'passage', 'above', 'street', 'stones', 'seen', 'world', 'starting', 'tilt', 'now', 'here', 'nowhere', 'where', 'were', 'going', 'visible', 'thing', 'thingless', 'we', 'came', 'world', 'thingless', 'we', 'leave', 'still', 'waves', 'bankrupt', 'pubs', 'suicides', 'eye', 'attracted', 'downward', 'down', 'avenue', 'malou', 'soon', 'killed', 'assimilated', 'lift','eyes', 'swallowed', 'saint', 'stone', 'st', 'antoine', 'anti', 'one', 'coine', 'coin', 'antconc', 'passersby', 'mind', 'touch', 'tram', 'ramming', 'heartbeat', 'wreath', 'breath', 'exhaust', 'host', 'hostage', 'traffic', 'clutter', 'gutter', 'utter', 'guttural', 'chocolate', 'store', 'flemish', 'gables', 'turkish', 'pizza', 'place', 'ace', 'room', 'doomed', 'con', 'connect', 'necked', 'echt', 'stuck', 'jam', 'headed', 'all', 'directions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7932854356288415line0 = ['brussels', 'rain', 'lost', 'in', 'the', 'minutes','hours', 'thrumming', 'on', 'the', 'brains','trams', 'on', 'tervuren', 'street', 'the', 'gate', 'of', 'cinquantenaire', 'slowly', 'looming', 'through', 'the', 'chestnut', 'trees']\n",
    "\n",
    "# 0.7630211685597917 line0 = ['brussels', 'rain', 'lost', 'in', 'the', 'minutes','hours', 'thrumming', 'on', 'your', 'brains']\n",
    "\n",
    "# 0.776370338753413 line0 = ['brussels', 'rain', 'lost', 'in', 'the', 'long', 'minutes', 'long','hours', 'thrumming', 'on', 'your', 'brains']\n",
    "\n",
    "# 0.8028070124351534 line0 = ['you', 'got', 'lost', 'in', 'the','brussels', 'rain', 'the', 'long', 'minutes', 'long','hours', 'thrumming', 'on', 'your', 'brains']\n",
    "\n",
    "# 0.807942003059286 line0 = ['you', 'got', 'lost', 'in', 'the','brussels', 'rain', 'the', 'long', 'minutes', 'and', 'hours', 'thrumming', 'on', 'your', 'brains']\n",
    "\n",
    "# 0.8213554789779391 line0 = ['you', 'got', 'lost', 'in', 'the','brussels', 'rain', 'the', 'long', 'minutes', 'and', 'hours', 'of', 'drops', 'thrumming', 'on', 'your', 'brains']\n",
    "\n",
    "# 0.8114431534834665 line0 = ['you', 'got', 'lost', 'in', 'the','brussels', 'rain', 'eating', 'away', 'at', 'your', 'nerves']\n",
    "\n",
    "# 0.7309086216450169 line0 = ['you', 'got', 'lost', 'in', 'the','brussels', 'rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thingless\n",
      "thingless\n",
      "antconc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.00547545e-02, -1.31303619e-02, -2.83823082e-02,  4.40135590e-02,\n",
       "       -3.46589276e-02,  6.06369008e-03,  1.74161521e-03, -2.48864032e-02,\n",
       "        6.92531857e-03,  2.97833488e-02,  1.51574625e-02,  7.69335571e-03,\n",
       "       -1.72233520e-02, -1.19005449e-02,  2.01595012e-02, -3.33612848e-02,\n",
       "       -2.11208877e-02, -6.01240780e-03,  7.71446720e-03,  4.85893336e-02,\n",
       "       -3.22472025e-02,  2.97685730e-02, -4.16689236e-02, -3.64222935e-02,\n",
       "       -2.11171391e-02, -1.11978547e-02, -4.47659741e-03, -6.33765830e-03,\n",
       "        4.03494891e-03,  1.92838304e-02, -2.61829020e-02,  5.35845970e-02,\n",
       "       -2.98444350e-02,  2.35333223e-02,  1.46818314e-02, -2.12395182e-02,\n",
       "        3.56956414e-03, -2.87006031e-02,  1.81089758e-03, -1.05279524e-02,\n",
       "        1.83347159e-02,  8.99918481e-03, -9.27684829e-03,  4.68453597e-03,\n",
       "        1.36936375e-02,  2.58961238e-02,  1.07507422e-02,  4.52908378e-03,\n",
       "        5.55842817e-03, -9.13369141e-03,  9.56861299e-03, -3.39959497e-02,\n",
       "        3.31530431e-03, -7.24137600e-04, -1.49601451e-02,  2.85802870e-02,\n",
       "        2.29312360e-03,  2.64843711e-02, -8.30029819e-03,  4.02670648e-02,\n",
       "        1.13604699e-02, -1.19693391e-02,  4.34592109e-02, -5.10564646e-03,\n",
       "       -8.56848728e-03, -1.92296698e-02, -7.13118337e-03,  2.03269500e-02,\n",
       "       -5.96277722e-03,  1.07635139e-02,  4.32765183e-03,  9.55620268e-03,\n",
       "        2.77358265e-02, -2.27069431e-02, -1.45640394e-02,  2.14964251e-02,\n",
       "        4.26039366e-02,  3.48475506e-02, -1.52035693e-02, -1.20957341e-02,\n",
       "        7.50654932e-03, -1.08139425e-02, -1.55736951e-02,  1.18119897e-02,\n",
       "       -3.10095128e-02,  8.52310738e-03, -1.82209972e-02,  1.43865569e-02,\n",
       "        7.07458254e-03, -3.05076094e-03, -3.80755005e-04, -2.33201597e-02,\n",
       "        5.32034219e-02, -2.97277947e-02,  3.10867338e-02,  2.00161384e-02,\n",
       "        2.90878148e-02,  2.88701090e-03, -1.88390727e-02,  1.57854787e-02,\n",
       "        1.35756068e-02, -2.69861706e-02,  1.72319203e-02, -9.00212639e-03,\n",
       "       -5.37322854e-03, -2.34718056e-02, -4.06823402e-02,  3.10391730e-02,\n",
       "        1.65009564e-02,  1.56518020e-03, -1.68002815e-03, -4.27740987e-04,\n",
       "       -7.23033152e-03, -9.25647429e-03, -1.84633814e-03, -3.07049740e-02,\n",
       "        3.05419959e-02,  5.71577851e-03, -1.85586511e-02,  1.85346748e-02,\n",
       "        5.99681654e-02,  5.24458669e-03, -1.54333739e-02,  5.00935487e-02,\n",
       "       -2.67586896e-04, -4.90141819e-03,  2.03063961e-02,  2.06035540e-02,\n",
       "        1.98070232e-02,  4.40853635e-02, -1.25222208e-02,  1.82880713e-02,\n",
       "       -1.76038318e-02,  1.61290718e-02,  1.04773102e-02,  9.78890932e-03,\n",
       "       -6.96244956e-03, -2.64612704e-03, -1.25597152e-02,  4.63145465e-02,\n",
       "       -1.44288057e-03,  2.33361552e-02,  2.19229880e-02,  6.52351385e-04,\n",
       "       -4.00923844e-03,  3.36401788e-02, -1.18833016e-02, -2.45482326e-02,\n",
       "       -5.38369612e-03,  1.61343774e-03,  4.19130022e-02, -5.19607097e-02,\n",
       "       -2.57868837e-02, -8.42501118e-03, -1.72291938e-02, -9.58204899e-03,\n",
       "       -1.26309745e-02, -2.46707612e-02,  3.10773449e-02, -6.32164563e-04,\n",
       "        4.73345486e-04,  2.87906985e-02, -3.62663287e-02,  2.40263240e-02,\n",
       "        2.45102436e-02,  1.47347156e-02, -2.20515630e-02, -1.91196439e-02,\n",
       "        5.11364797e-02,  3.32879935e-03, -4.34837136e-02, -2.42509531e-02,\n",
       "       -4.92453861e-02, -8.20558815e-03, -2.47251043e-02,  5.42002456e-02,\n",
       "       -4.14992335e-04,  1.33052091e-02,  1.84899918e-03, -2.85206804e-02,\n",
       "        2.57028027e-02, -2.04104036e-02,  1.70965126e-02,  7.06883037e-03,\n",
       "       -5.87105340e-03,  4.21982116e-02, -7.81409271e-03, -2.03583022e-02,\n",
       "        3.48632522e-02, -1.76163775e-02,  1.24276934e-02, -5.41513733e-02,\n",
       "        2.01265344e-02,  4.57936637e-02, -2.81830880e-02, -1.65053896e-02,\n",
       "       -8.26606964e-03,  1.57886258e-02, -6.46587093e-02, -2.01871794e-02,\n",
       "        1.06154371e-02,  4.72226364e-03,  1.72779589e-02,  1.93747660e-03,\n",
       "        6.36188707e-03, -5.10469091e-02, -3.47923652e-02,  4.67699381e-03,\n",
       "        3.17741643e-02,  7.64913108e-03,  9.31484276e-02,  2.64381937e-03,\n",
       "        7.18796775e-03, -2.37030927e-02,  1.38443568e-02, -9.96401825e-03,\n",
       "       -1.01311866e-02, -6.84726609e-02, -2.73259398e-02,  8.54133853e-03,\n",
       "       -2.36140759e-02, -9.55198906e-03, -4.11183352e-02,  3.20263964e-02,\n",
       "       -3.19735456e-04,  1.05897454e-02, -2.29123964e-02, -1.84480076e-02,\n",
       "        1.17620685e-02, -9.79867228e-03,  8.12105419e-03,  1.40330476e-02,\n",
       "        1.23325292e-02,  9.20593795e-03,  1.27735084e-02,  4.82169278e-03,\n",
       "        1.36469707e-02,  2.32573464e-02,  1.14938329e-03,  1.23824894e-03,\n",
       "       -5.55999458e-03,  1.04865794e-02,  1.21111060e-02,  1.87024280e-02,\n",
       "       -2.26307975e-02,  3.48990547e-02,  1.12449561e-02,  5.44818628e-03,\n",
       "       -1.85878729e-02, -8.86677211e-03,  1.83475090e-02, -1.31127629e-02,\n",
       "       -3.10707381e-02, -1.63761053e-02,  1.35523910e-02,  1.24348469e-02,\n",
       "       -9.01992025e-03, -5.08524345e-02, -6.38926961e-03, -1.86472497e-03,\n",
       "        1.65914687e-04,  2.71110230e-02,  1.06261314e-02, -2.60131120e-02,\n",
       "       -4.71434535e-02, -3.17779056e-02, -2.71289109e-02,  1.62390249e-02,\n",
       "       -1.03469073e-02, -1.29113473e-02, -4.33160488e-02, -8.33754696e-03,\n",
       "        1.67711371e-02,  1.88322498e-02, -4.11189433e-02, -3.07068044e-02,\n",
       "        3.70867260e-02,  5.12138180e-02,  2.50512782e-04,  3.65310782e-03,\n",
       "        2.28581303e-02, -9.23389131e-04, -1.56202272e-03,  2.54947750e-03,\n",
       "        3.06270735e-02,  1.38029190e-02,  2.55490013e-02,  2.24608368e-02,\n",
       "       -1.43205928e-02,  2.69611466e-02,  8.91040140e-03, -7.83283058e-03,\n",
       "        9.32357375e-05,  3.26069098e-03, -6.40769381e-03, -1.69202055e-02,\n",
       "       -2.40357478e-02,  3.42924660e-02,  2.51309239e-02,  2.37764705e-02])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#vector(line0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9474654303367803\n"
     ]
    }
   ],
   "source": [
    "print (cos_sim(vector(line0), vect_en[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "line0 = ['road', 'end', 'cobble', 'drunkenness', 'nowhere', 'go', 'keep', 'on', 'eudore', 'pirmez', 'like', 'past', 'clouds', 'tight', 'passage', 'above', 'street', 'stones', 'seen', 'world', 'starting', 'tilt', 'now', 'here', 'nowhere', 'where', 'were', 'going', 'visible', 'thing', 'thingless', 'we', 'came', 'world', 'thingless', 'we', 'leave', 'still', 'waves', 'bankrupt', 'pubs', 'suicides', 'eye', 'attracted', 'downward', 'down', 'avenue', 'malou', 'soon', 'killed', 'assimilated', 'lift','eyes', 'swallowed', 'saint', 'stone', 'st', 'antoine', 'anti', 'one', 'le','coin', 'my','coins', 'antconc', 'passersby', 'mind', 'touch', 'tram', 'ramming', 'heartbeat', 'wreath', 'breath', 'exhaust', 'host', 'hostage', 'traffic', 'clutter', 'gutter', 'utter', 'guttural', 'chocolate', 'store', 'flemish', 'gables', 'turkish', 'pizza', 'place', 'ace', 'room', 'doomed', 'con', 'connect', 'necked', 'echt', 'stuck', 'jam', 'headed', 'all', 'directions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.949095145932522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(line0), vect_en[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lineX = ['your', 'org', 'organ', 'organon', 'no', 'no', 'skin', 'mem', 'brain', 'membrane', 'rains', 'spasm', 'spastic', 'screams', 'on', 'iphone', 'screen', 'spawning', 'awning', 'dawn', 'chris', '$', 'trains', 'rains', 'cage', 'cagey', 'christ', 'st', 'mone', 'money', 'y', 'suis', 'en', 'train', 'wall', 'ache', 'wallachia', 'for', 'm', 'bandied', 'we', 'unda', 'munda', 'mundaneum', 'mon', 'mons', 'moans', 'soon', 'monsoon', 'con', 'fence', 'conference', 'in', 'our', 'reference', 'pneuma', 'cult', 'ur', 'culture', 'shared', 'me', 'mode', 'de', 'res', 'mores', 'memories', 's', 'i', 'cent', 're', 'centre', 'centuries', 'on', 'pa', 'paper', 'her', 'sphere', 'ph', 'prow', 'cutting', 's', 'spires', 'pyres', 'am', 'empires', 'out', 'of', 'the', 'pic', 'moderate', 'modern', 'e', 'very', 'thin', 'g', 'everything', 'war', 'all', 'warhol', 'blood', 'wave', 'wag', 'ave', 'vero', 'ro', 'roaring', 'go', 'out', 'gout', 'outlet', 'otlet', 'any', 'body', 'anybody', 'neu', 'eu', 'let', 'dying', 'knew', 'touch', 'touching', 'she', '$','he', 'had', 'put', 'in', 'be', 'bell', 'for', '€', 'fore', 'before', 'p', 'ass', 'passing', 'anybody', 'i', 'know', 'i', 'will', 'die', 'if', 'i', 'd', 'dont', 'ont', 'k', 'now', 'know', 'never', 'will', 'pont', 'inside', 'y', 'our', 'your', 'vague', 'ue', 'vagina', 'hear', 't', 'heart', 'be', 'at', 'beat'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0: 'charles_wright_apologia_pro_vita_sua.txt', 1: 'elke_de_rijcke_deffectiveness.txt', 2: 'place_babylonians-trans-jared-pearce_high-priest-prayer.txt', 3: 'place_david-baker_weed_granville-ohio-&-ontario-&-illinois-&-eerie-&-carolina-&-asia.txt', \n",
    "# 4: 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 5: 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', \n",
    "# 6: 'sharon_olds_the_knowing.txt', 7: 'ted_hughes_harvest_moon.txt', 8: 'veronique_bergen_trans_margento_wolves.txt', 9: 'w_h_auden_brussels_in_winter.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9516638825863778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9439541587545881\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8931870013728541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896654653885662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9364661073953374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934262393351409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332125354735691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8873837204024809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379738899189959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9152198726911038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(lineX), vect_en[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9170378341806269\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (cos_sim(vector(line0), vect_en[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lineY = ['road', 'end', 'drunkenness','cobble', 'stone', 'co', 'bible', 'nowhere', 'concept', 'go', 'keep', 'on', 'eudore', 'pirmez', 'eu', 'do', 're', '©', 'door', 'pir', 'mez','like', 'past', 'clouds', 'loud', 'pt', 'ah', 'ptah', 'tight', 'passage', 'co', 'nc', 'err', 't', 'concert', 'ierta', 'vœrtex','above', 'street', 'stones', 'seen', 'world', 'starting', 'tilt', 'now', 'here', 'nowhere', 'where', 'were', 'going', 'visible', 'thing', 'thingless', 'we', 'came', 'world', 'thingless', 'we', 'leave', 'still', 'waves', 'bankrupt', 'pubs', 'bank', 'e', 'rupt','suicides', 'eye', 'attracted', 'downward', 'down', 'avenue', 'malou', 'soon', 'killed', 'assimilated', 'lift','eyes', 'swallowed', 'ore', 'well', 'orwell', 'sui', 'dna', 'saint', 'stone', 'st', 'antoine', 'anti', 'one', 'tone', 'le','coin', 'gni', 'ht', 'sel', 'sell', 'my','coins', 'antconc', 'passerby', 'mind', 'touch', 'tram', 'ramming', 'heartbeat', 'wreath', 'breath', 'exhaust', 'host', 'hostage', 'traffic', 'stage', 'clutter', 'gutter', 'utter', 'guttural', 'chocolate', 'store', 'flemish', 'gables', 'turkish', 'pizza', 'place', 'ace', 'room', 'doomed', 'con', 'connect', 'necked', 'echt', 'stuck', 'jam', 'headed', 'all', 'directions']\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9526536123777063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(cos_sim(vector(lineY), vect_en[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9175443484963732\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(vector(lineY), vect_en[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.8717047642779274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(cos_sim(vector(lineY), vect_en[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9098063939329298\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(vector(lineY), vect_en[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9331610618811681\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(vector(lineY), vect_en[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.8930866309552881\n",
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9096865229728374\n",
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9082715666657221\n",
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9352446146748088\n",
      "ierta\n",
      "vœrtex\n",
      "thingless\n",
      "thingless\n",
      "antconc\n",
      "0.9367645010963497\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(vector(lineY), vect_en[5]))\n",
    "print(cos_sim(vector(lineY), vect_en[6]))\n",
    "print(cos_sim(vector(lineY), vect_en[7]))\n",
    "print(cos_sim(vector(lineY), vect_en[8]))\n",
    "print(cos_sim(vector(lineY), vect_en[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now X = 1 and Y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lineZ = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', 'bare', 'barely', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', '©', 'crowd', 'rowdy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degnity\n",
      "espritzer\n",
      "0.8958725143382601\n",
      "degnity\n",
      "espritzer\n",
      "0.8684293637069912\n",
      "degnity\n",
      "espritzer\n",
      "0.847580217998099\n",
      "degnity\n",
      "espritzer\n",
      "0.8637697915046306\n",
      "degnity\n",
      "espritzer\n",
      "0.8674895809499273\n",
      "degnity\n",
      "espritzer\n",
      "0.856659886315774\n",
      "degnity\n",
      "espritzer\n",
      "0.8548618540780187\n",
      "degnity\n",
      "espritzer\n",
      "0.8613722166540756\n",
      "degnity\n",
      "espritzer\n",
      "0.8970713571524133\n",
      "degnity\n",
      "espritzer\n",
      "0.8830451515792974\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(vector(lineZ), vect_en[0]))\n",
    "print(cos_sim(vector(lineZ), vect_en[1]))\n",
    "print(cos_sim(vector(lineZ), vect_en[2]))\n",
    "print(cos_sim(vector(lineZ), vect_en[3]))\n",
    "print(cos_sim(vector(lineZ), vect_en[4]))\n",
    "print(cos_sim(vector(lineZ), vect_en[5]))\n",
    "print(cos_sim(vector(lineZ), vect_en[6]))\n",
    "print(cos_sim(vector(lineZ), vect_en[7]))\n",
    "print(cos_sim(vector(lineZ), vect_en[8]))\n",
    "print(cos_sim(vector(lineZ), vect_en[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lineZ1 = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', '@', 'gura', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', '@', 'bare', 'barely', 'euro', 'barley', '@', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', 'copyright', 'crowd', 'rowdy', 'en', 'co', 'un', 'ter', 'term', 'therm', 'terra', 'ra', 'is', 'on', 'unison', 'at', 'temp', 't', 'attempt', 'on', 'toll', 'ontology', 'st', 'ore', 'store','temple', 'euro', 'dollar', 'pound', 'pro', 'leprosy', 'touch', 'less', 'touchless', 'ga', 'gang', 'ng', 'b', 'bang', 'gangbang', 'angst', 'st', 'store', 'orgy', 'euro', 'pound', 'sy', 'gy', 'syzygy', 'Z', 'integer', 'number','set', 'dollar', 'Zy', 'G', 'G-number', 'Grahams-number', 'gaga', 'st', 'stumble', 'humble', 'h', 'registered-mark', 'neck', 'O', 'logy', 'gagalogy', 'gynecology', 'of', 'traf', 'raf', 'registered-mark', 'fic', 'fict', 'beatitude', 'beat', 'eat', 'it', 'traffic', 'finitude', 'copyright', 'registered-mark', 'registered-mark', 'euro', 'trade-mark', 'copyright', 'dollar', 'euro','beatitudes', 'finitudes', 'ud', 'ude', 'stare', 'a', 're', 'stud', 'angels', 'else', 'els', 'euro', 'elsewhere', 'we', 'are', 'where', 'h', 'a', 'o', 'or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8975539035830501\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8646927981626514\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.843999896789601\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8604117336602712\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8679394761445945\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8489275305176048\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8480756736757716\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8614512796613774\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8960247550547458\n",
      "degnity\n",
      "espritzer\n",
      "Z\n",
      "Zy\n",
      "G\n",
      "G-number\n",
      "Grahams-number\n",
      "registered-mark\n",
      "O\n",
      "gagalogy\n",
      "registered-mark\n",
      "registered-mark\n",
      "registered-mark\n",
      "trade-mark\n",
      "finitudes\n",
      "0.8820183892586599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(0, cos_sim(vector(lineZ1), vect_en[0]))\n",
    "print(1, cos_sim(vector(lineZ1), vect_en[1]))\n",
    "print(2, cos_sim(vector(lineZ1), vect_en[2]))\n",
    "print(3, cos_sim(vector(lineZ1), vect_en[3]))\n",
    "print(4, cos_sim(vector(lineZ1), vect_en[4]))\n",
    "print(5, cos_sim(vector(lineZ1), vect_en[5]))\n",
    "print(6, cos_sim(vector(lineZ1), vect_en[6]))\n",
    "print(7, cos_sim(vector(lineZ1), vect_en[7]))\n",
    "print(8, cos_sim(vector(lineZ1), vect_en[8]))\n",
    "print(9, cos_sim(vector(lineZ1), vect_en[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LineZ1 = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', '@', 'gura', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', '@', 'bare', 'barely', 'euro', 'barley', '@', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', 'copyright', 'crowd', 'rowdy', 'en', 'co', 'un', 'ter', 'term', 'therm', 'terra', 'ra', 'is', 'on', 'unison', 'at', 'temp', 't', 'attempt', 'on', 'toll', 'ontology', 'st', 'ore', 'store','temple', 'euro', 'dollar', 'pound', 'pro', 'leprosy', 'touch', 'less', 'touchless', 'ga', 'gang', 'ng', 'b', 'bang', 'gangbang', 'angst', 'st', 'store', 'orgy', 'euro', 'pound', 'sy', 'gy', 'syzygy', 'Z', 'integer', 'number','set', 'dollar', 'Zy', 'G', 'G-number', 'Grahams-number', 'Graham', 'number', 'gaga', 'st', 'stumble', 'humble', 'h', '®', 'neck', 'O', 'logy', 'gagalogy', 'gynecology', 'of', 'traf', 'raf', '®', 'fic', 'fict', 'beatitude', 'beat', 'eat', 'it', 'traffic', 'finitude', 'copyright', '®', '®', 'euro', 'tm', 'copyright', 'dollar', 'euro','beatitudes', 'finitudes', 'ud', 'ude', 'stare', 'a', 're', 'stud', 'angels', 'else', 'els', 'euro', 'elsewhere', 'we', 'are', 'where', 'h', 'a', 'o', 'or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineZ1 = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', '@', 'gura', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', '@', 'bare', 'barely', 'euro', 'barley', '@', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', 'copyright', 'crowd', 'rowdy', 'en', 'co', 'un', 'ter', 'term', 'therm', 'terra', 'ra', 'is', 'on', 'unison', 'at', 'temp', 't', 'attempt', 'on', 'toll', 'ontology', 'st', 'ore', 'store','temple', 'euro', 'dollar', 'pound', 'pro', 'leprosy', 'touch', 'less', 'touchless', 'ga', 'gang', 'ng', 'b', 'bang', 'gangbang', 'angst', 'st', 'store', 'orgy', 'euro', 'pound', 'sy', 'gy', 'syzygy', 'z', 'integer', 'number','set', 'dollar', 'zy', 'g', 'g-number', 'grahams-number', 'graham', 'number', 'gaga', 'st', 'stumble', 'humble', 'h', '®', 'neck', 'o', 'logy', 'gagalogy', 'gynecology', 'of', 'traf', 'raf', '®', 'fic', 'fict', 'beatitude', 'beat', 'eat', 'it', 'traffic', 'finitude', 'copyright', '®', '®', 'euro', 'tm', 'copyright', 'dollar', 'euro','beatitudes', 'finitudes', 'ud', 'ude', 'stare', 'a', 're', 'stud', 'angels', 'else', 'els', 'euro', 'elsewhere', 'we', 'are', 'where', 'h', 'a', 'o', 'or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "0 0.8959187603026576\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "1 0.8646043837913708\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "2 0.8439660383840966\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "3 0.8573924497713336\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "4 0.8676085016956236\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "5 0.8487293464381855\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "6 0.846468839649542\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "7 0.8575238298258451\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "8 0.8934175394892132\n",
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "9 0.877797386084055\n"
     ]
    }
   ],
   "source": [
    "print(0, cos_sim(vector(lineZ1), vect_en[0]))\n",
    "print(1, cos_sim(vector(lineZ1), vect_en[1]))\n",
    "print(2, cos_sim(vector(lineZ1), vect_en[2]))\n",
    "print(3, cos_sim(vector(lineZ1), vect_en[3]))\n",
    "print(4, cos_sim(vector(lineZ1), vect_en[4]))\n",
    "print(5, cos_sim(vector(lineZ1), vect_en[5]))\n",
    "print(6, cos_sim(vector(lineZ1), vect_en[6]))\n",
    "print(7, cos_sim(vector(lineZ1), vect_en[7]))\n",
    "print(8, cos_sim(vector(lineZ1), vect_en[8]))\n",
    "print(9, cos_sim(vector(lineZ1), vect_en[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lineZ2 = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', '@', 'gura', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'wave', 'wavy', 'wavre', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', '@', 'bare', 'barely', 'euro', 'barley', '@', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', 'copyright', 'crowd', 'rowdy', 'en', 'co', 'un', 'ter', 'term', 'therm', 'terra', 'ra', 'is', 'on', 'unison', 'at', 'temp', 't', 'attempt', 'on', 'toll', 'ontology', 'st', 'ore', 'store','temple', 'euro', 'dollar', 'pound', 'pro', 'leprosy', 'touch', 'less', 'touchless', 'ga', 'gang', 'ng', 'b', 'bang', 'gangbang', 'angst', 'st', 'store', 'orgy', 'euro', 'pound', 'sy', 'gy', 'syzygy', 'z', 'integer', 'number','set', 'dollar', 'zy', 'g', 'g-number', 'grahams-number', 'graham', 'number', 'gaga', 'st', 'stumble', 'humble', 'h', '®', 'neck', 'o', 'logy', 'gagalogy', 'gynecology', 'of', 'traf', 'raf', '®', 'fic', 'fict', 'beatitude', 'beat', 'eat', 'it', 'traffic', 'finitude', 'copyright', '®', '®', 'euro', 'tm', 'copyright', 'dollar', 'euro','beatitudes', 'finitudes', 'ud', 'ude', 'stare', 'a', 're', 'stud', 'angels', 'else', 'els', 'euro', 'elsewhere', 'we', 'are', 'where', 'h', 'a', 'o', 'or', 'food', 'look', 'work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "2 0.8483009326847178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(2, cos_sim(vector(lineZ2), vect_en[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lineZ3 = ['ever', 'you', 'every', 'e', 'ver', 'x', 'ile', 'exile', 'ou', 'gur', 'u', 'guru', '@', 'gura', 'r', 'age', 'rage', 'gnit', 'y', 'degnity', 'dignity', 'wave', 'wavy', 'wavre', 'p', 'lace', 'place', 'f', 'at', 'fat', 'her', 'father', 'sc', 'y', 'the', 'scythe', 'scatter', 'fami', 'famine', 'family', 'at', 'pat', 'patience', 'temp', 'science', 'attempt', 's', 'id', 'e', 'side', 'ion', 'ignition', 'tube', 'youtube', 'walk', 'sidewalk', 'tm', 'extr', 'ex', 'extr', 'extra', 'bar', '@', 'bare', 'barely', 'euro', 'barley', '@', 'c', 'limb', 'climb', 'climax', 'ch', 'err', 'ub', 'ubu', 'cherub', 'err','terra', 'gura', 'esp', 'ritz', 'esprit', 'spritzer', 'espritzer', 'thé', 'uterus', 'uber', 'bus', 'limax', 'wom', 'woman', 'womb', 'limn', 'limes', 'sexless', 'bless', 'copyright', 'crowd', 'rowdy', 'en', 'co', 'un', 'ter', 'term', 'therm', 'terra', 'ra', 'is', 'on', 'unison', 'at', 'temp', 't', 'attempt', 'on', 'toll', 'ontology', 'st', 'ore', 'store','temple', 'euro', 'dollar', 'pound', 'pro', 'leprosy', 'touch', 'less', 'touchless', 'ga', 'gang', 'ng', 'b', 'bang', 'gangbang', 'angst', 'st', 'store', 'orgy', 'euro', 'pound', 'sy', 'gy', 'syzygy', 'z', 'integer', 'number','set', 'dollar', 'zy', 'g', 'g-number', 'grahams-number', 'graham', 'number', 'gaga', 'st', 'stumble', 'humble', 'h', '®', 'neck', 'o', 'logy', 'gagalogy', 'gynecology', 'of', 'traf', 'raf', '®', 'fic', 'fict', 'beatitude', 'beat', 'eat', 'it', 'traffic', 'finitude', 'copyright', '®', '®', 'euro', 'tm', 'copyright', 'dollar', 'euro','beatitudes', 'finitudes', 'ud', 'ude', 'stare', 'a', 're', 'stud', 'angels', 'else', 'els', 'euro', 'elsewhere', 'we', 'are', 'where', 'h', 'a', 'o', 'or', 'food', 'look', 'work', 'mother', 'sisters', 'other', 'h', 'sys', 'system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degnity\n",
      "espritzer\n",
      "g-number\n",
      "grahams-number\n",
      "gagalogy\n",
      "finitudes\n",
      "2 0.852218869490196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(2, cos_sim(vector(lineZ3), vect_en[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
